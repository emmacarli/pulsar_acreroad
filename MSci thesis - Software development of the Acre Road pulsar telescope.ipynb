{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSci thesis by 2185343, 2020 - see report for details.\n",
    "\n",
    "This is a summary version of all the code written for this project. Some scripts are left out for clarity, notably the FFT transform section, plotting scripts, Gaussian fitting of the template,The whole project is available on GitHub under my full name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive plots, use:\n",
    "#%matplotlib ipympl\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.stats import sigma_clip\n",
    "from progress.bar import Bar #https://pypi.org/project/progress/\n",
    "from astropy.time import Time\n",
    "from My_Functions.update_text import update_text #taken from https://stackoverflow.com/questions/49742962/how-to-insert-text-at-line-and-column-position-in-a-file\n",
    "from shutil import copyfile\n",
    "import os, re, time, subprocess, glob\n",
    "\n",
    "# Make text larger for readability on graphs\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['font.size'] = 14\n",
    "# Set same fonts as my LaTeX document\n",
    "plt.rcParams['font.family'] = 'STIXGeneral' \n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "# Other plotting params\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set known variables from observations\n",
    "\n",
    "sampling_period = 2e-3 #2 milliseconds\n",
    "one_second_in_datapoints = int(1/sampling_period) #500 datapoints = 1 second of recording\n",
    "one_minute_in_datapoints= one_second_in_datapoints * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "\n",
    "path_to_raw_files = '/home/emma/Desktop/Raw_Datafiles/'\n",
    "path_to_cleaned_files = '/home/emma/Desktop/Cleaned_Data/'\n",
    "path_to_folded_profiles =  '/home/emma/Desktop/pulsardataprep_acreroad/Folding/PRESTO_Folded_Profiles/'\n",
    "path_to_template_profile = '/home/emma/Desktop/pulsardataprep_acreroad/Folding/Jodrell_Template_Profile_I-Q_PRESTO_Gaussian_fit.gaussians' #this was generated using PRESTO's pygaussfit.py on the Jodrell Bank template for this pulsar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files needed in the working directory: Template_PRESTO_inf_file.txt , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of raw files' paths in my computer\n",
    "raw_files = glob.glob(path_to_raw_files+'*-PSRB0329-2ms-sampling-dd.dat') #this suffix is added at recording\n",
    "\n",
    "bar = Bar('Processing...', max=len(raw_files), fill='\\U0001F4E1', suffix = '%(percent).1f%% - %(eta)ds') #create a progress bar\n",
    "bar.check_tty = False\n",
    "\n",
    "# Go through each raw file\n",
    "for raw_file in raw_files: \n",
    "    start_time_GPS = float(raw_file[len(path_to_raw_files):-len('-PSRB0329-2ms-sampling-dd.dat')]) #the start of the filename gives the beginning of the recording time in GPS format\n",
    "    bar.next()\n",
    "      \n",
    "    #If this file has already been cleaned, skip it and go to the next in raw_files\n",
    "    if glob.glob(path_to_cleaned_files+str(start_time_GPS)+'_cleaned.dat') != []:\n",
    "        print(str(start_time_GPS)+' already cleaned.')\n",
    "        continue\n",
    "    \n",
    "    # Load in the raw data\n",
    "    handle_raw_file = open(raw_file, mode='rb') \n",
    "    data_raw = np.fromfile(handle_raw_file,'f4')\n",
    "    \n",
    "    # Crop the raw file to a length that is an integer number of minutes\n",
    "    # so that can perform operations over one minute / one second intervals\n",
    "    number_of_one_minute_intervals = int( np.floor( len(data_raw) / one_minute_in_datapoints ))\n",
    "    data1 = data_raw[0: number_of_one_minute_intervals * one_minute_in_datapoints] #step 1 of cleaning\n",
    "    \n",
    "    # Remove median of whole dataset\n",
    "    data2 = data1 - np.median(data1) #step 2 of cleaning\n",
    "    \n",
    "    # Median removal over 1 second intervals (not a proper filter with sliding window!)\n",
    "    median_filter_1s = np.zeros(len(data2))\n",
    "    number_of_one_second_intervals = number_of_one_minute_intervals * 60\n",
    "\n",
    "    second_long_chunks_starting_points = np.linspace(0, len(data2) - one_second_in_datapoints , number_of_one_second_intervals , dtype='int')\n",
    "\n",
    "    for second_long_chunk_start in second_long_chunks_starting_points :\n",
    "        second_long_chunk_end =  second_long_chunk_start + one_second_in_datapoints\n",
    "        \n",
    "        median_filter_1s[second_long_chunk_start : second_long_chunk_end] = np.median(data2[second_long_chunk_start : second_long_chunk_end]) \n",
    "\n",
    "    data3 = data2 - median_filter_1s #step 3 of cleaning\n",
    "\n",
    "    # Sigma clip over 1 minute intervals\n",
    "    data4 = np.zeros(len(data3))\n",
    "    \n",
    "    minute_long_chunks_starting_points = np.linspace(0, len(data3) - one_minute_in_datapoints , number_of_one_minute_intervals , dtype='int')\n",
    "    \n",
    "    for minute_long_chunk_start in minute_long_chunks_starting_points :\n",
    "        minute_long_chunk_end =  minute_long_chunk_start + one_minute_in_datapoints\n",
    "        \n",
    "        sigma_masked_array = sigma_clip(data3[minute_long_chunk_start : minute_long_chunk_end], sigma=5, cenfunc='median', masked=True) #this is a NumPy MaskedArray object\n",
    "        sigma_masked_array.set_fill_value(0.0)\n",
    "        \n",
    "        data4[minute_long_chunk_start : minute_long_chunk_end] = sigma_masked_array.filled()\n",
    "        \n",
    "    # STD divide to whiten data over a 1 minute interval\n",
    "        data_cleaned = np.zeros(len(data4))\n",
    "        \n",
    "        for minute_long_chunk_start in minute_long_chunks_starting_points :\n",
    "    \n",
    "            minute_long_chunk_end =  minute_long_chunk_start + one_minute_in_datapoints\n",
    "            \n",
    "            if np.std(data4[minute_long_chunk_start : minute_long_chunk_end]) != 0 :\n",
    "                data_cleaned[minute_long_chunk_start : minute_long_chunk_end] = data4[minute_long_chunk_start : minute_long_chunk_end]/np.std(data4[minute_long_chunk_start : minute_long_chunk_end]) #4th and last step of cleaning\n",
    "        \n",
    "    # Write the cleaned file in the same way as a raw file, so can be opened the same way\n",
    "    handle_file_cleaned = open(path_to_cleaned_files+str(start_time_GPS)+'_cleaned.dat', 'wb')\n",
    "    data_cleaned_binary = np.array(data_cleaned, 'f4')\n",
    "    data_cleaned_binary.tofile(handle_file_cleaned)\n",
    "    \n",
    "    # Close files\n",
    "    handle_file_cleaned.close()\n",
    "    handle_raw_file.close()\n",
    "\n",
    "\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folding, SNR and TOA computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of phase bins in the folded profiles and in the total profile, it has to be a power of two, for FFTFIT.\n",
    "# Otherwise, for folding, PRESTO defaults to the number of sampling bins which correspond to one folded period, in our case about 64, and for the total profile, it defaults to 128.\n",
    "number_of_profile_bins = '512' #I chose this amount to limit computation time and \n",
    "\n",
    "# Start a log\n",
    "log_handle = open('PRESTO_Fold_SNR_TOA.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out time span of the available observations (for information and context only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_files_paths = sorted(glob.glob(path_to_cleaned_files +'*_cleaned.dat'))\n",
    "minimum_GPS_time = Time(float(cleaned_files_paths[0][len(path_to_cleaned_files):-len('_cleaned.dat')]), format='gps') #this is the first date at which I start having observations, extracted from the first file name of the ordered cleaned files.\n",
    "log_handle.write('The observations start on ' + minimum_GPS_time.iso+'\\n')\n",
    "maximum_GPS_time = Time(float(cleaned_files_paths[len(cleaned_files_paths)-1][len(path_to_cleaned_files):-len('_cleaned.dat')]), format='gps') #this is the last observation date\n",
    "log_handle.write('The observations end on ' + maximum_GPS_time.iso+'\\n')\n",
    "observations_span = maximum_GPS_time - minimum_GPS_time\n",
    "log_handle.write('The observations span '+str(observations_span.jd)+' days. \\n')\n",
    "log_handle.write('\\n \\n \\n \\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESTO_TOA_SNR_handle = open('PRESTO_TOAs_SNRs.txt' , 'w') #create a new file for saving PRESTO's TOA and SNR output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulsar topocentric frequency predictions, folding, SNR and TOA computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = Bar('Processing...', max=len(cleaned_files_paths), fill='\\U0001F4E1', suffix = '%(percent).1f%% - %(eta)ds') #create a progress bar\n",
    "bar.check_tty = False\n",
    "\n",
    "for cleaned_file_path in cleaned_files_paths:\n",
    "    \n",
    "    # Find out the start time and length of the observation\n",
    "    \n",
    "    start_time_GPS = float(cleaned_file_path[len(path_to_cleaned_files):-len('_cleaned.dat')])\n",
    "    start_time_GPS_astropy = Time(start_time_GPS, format='gps') \n",
    "    log_handle.write('Observation starting on GPS time '+str(start_time_GPS)+' i.e. '+str(start_time_GPS_astropy.iso)+'\\n')\n",
    "    handle_cleaned_file = open(cleaned_file_path)\n",
    "    data_cleaned = np.fromfile(handle_cleaned_file,'f4') #cleaned dataset\n",
    "    total_seconds_cleaned = len(data_cleaned)*sampling_period #total seconds in dataset\n",
    "    total_hours_cleaned =  total_seconds_cleaned / 3600 #total hours in dataset\n",
    "    total_hours_rounded_cleaned = np.ceil(total_hours_cleaned) #total rounded hours in dataset\n",
    "    \n",
    "    log_handle.write(' \\n ') \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Generate the TEMPO polynomial coefficients for this observation\n",
    "    \n",
    "    # Create the custom TEMPO command for this file\n",
    "    TEMPO_command = 'tempo -ZOBS=AR -ZFREQ=407.5 -ZTOBS='+str(total_hours_rounded_cleaned)+' -ZSTART='+str(start_time_GPS_astropy.mjd)+'  -ZNCOEFF=15 -ZSPAN='+str(int(total_hours_rounded_cleaned))+'H -f J0332+5434_initial_parameters.par'\n",
    "    log_handle.write('TEMPO command: ' + TEMPO_command+'\\n')\n",
    "\n",
    "    terminal_TEMPO_run = subprocess.run(TEMPO_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    log_handle.write(terminal_TEMPO_run.stdout.decode('utf-8')+'\\n') #copy the TEMPO output to the log\n",
    "    \n",
    "    # Here I add the name of the pulsar because for some reason, TEMPO does not put it in, and it is needed in PRESTO\n",
    "    polyco_handle = open('polyco.dat','r+')\n",
    "    polyco_contents = polyco_handle.read()\n",
    "    polyco_handle.seek(0,0)\n",
    "    polyco_handle.write('0332+5434  ')\n",
    "    polyco_handle.close()\n",
    "    \n",
    "    # When doing TOA finding later in this for loop, PRESTO looks for the polycos along with the folded profile\n",
    "    copyfile('polyco.dat', path_to_folded_profiles+str(start_time_GPS)+'_PSR_0332+5434.pfd.polycos')\n",
    "    \n",
    "    log_handle.write(' \\n ') \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    # Create PRESTO .inf file for this observation\n",
    "    # This file contains information about the observation essential to the PRESTO folding software\n",
    "    # Modify a template inf file. It has empty slots that need filled.\n",
    "    \n",
    "    copyfile('Template_PRESTO_inf_file.txt', 'current_PRESTO_inf_file.inf') #this creates an empty template file, and overwrites the previous instance of it\n",
    "    \n",
    "    # Write the datafile name, without suffix\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=1, column=44, text=cleaned_file_path[:-len('.dat')])\n",
    "    \n",
    "    # Write the observation start MJD\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=8, column=44, text=str(start_time_GPS_astropy.mjd))\n",
    "    \n",
    "    # Write the number of bins in the time series\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=10, column=44, text=str(len(data_cleaned)))\n",
    "    \n",
    "    # PRESTO will look for this information file in the same location as the data file, with the same name. This will overwrite any previous instances.\n",
    "    copyfile('current_PRESTO_inf_file.inf', cleaned_file_path[:-len('dat')]+'inf')\n",
    "    # PRESTO also looks for an inf file along with the folded profiles later, when doing TOA finding\n",
    "    copyfile('current_PRESTO_inf_file.inf', path_to_folded_profiles+str(start_time_GPS)+'_cleaned.inf')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform the fold for this observation\n",
    "    \n",
    "    PRESTO_fold_command = 'prepfold -nosearch -absphase -polycos polyco.dat -psr 0332+5434 -double -noxwin -n '+number_of_profile_bins+' -o '+path_to_folded_profiles+str(start_time_GPS)+' '+cleaned_file_path)\n",
    "    log_handle.write('PRESTO fold command: ' + PRESTO_fold_command+'\\n')\n",
    "    \n",
    "    terminal_fold_run = subprocess.run(PRESTO_fold_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    log_handle.write(terminal_fold_run.stdout.decode('utf-8')+'\\n')\n",
    "    \n",
    "    log_handle.write(' \\n ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute TOA, TOA error and SNR using FFTFIT\n",
    "    PRESTO_TOA_SNR_command = 'get_TOAs.py -f -n 1 -g '+path_to_template_profile+' '+path_to_folded_profiles+str(start_time_GPS)+'_PSR_0332+5434.pfd'\n",
    "    \n",
    "    log_handle.write('PRESTO TOA command: ' + PRESTO_TOA_SNR_command+'\\n')\n",
    "    log_handle.write('Output in PRESTO_TOAs.txt . \\n')\n",
    "    \n",
    "    terminal_TOA_SNR_run = subprocess.run(PRESTO_TOA_SNR_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    PRESTO_TOA_SNR_handle.write(terminal_TOA_SNR_run.stdout.decode('utf-8')+'\\n')\n",
    "    #I currently get a warning saying that the .inf file cannot be found. I wonder why! It's OK, because the information taken from the inf file in the case of TOA finding defaults to the right values: DM 0 and number of channels 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    log_handle.write('\\n \\n \\n \\n \\n')\n",
    "    bar.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-loop cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar.finish()\n",
    "time.sleep(2)\n",
    "print('Polycos, folding and retrieving TOAs took '+str(bar.elapsed-2)+' s.')\n",
    "os.remove('current_PRESTO_inf_file.inf') #inf file of the last processed observation\n",
    "os.remove('polyco.dat') #this was the predictor file of the last processed observation\n",
    "PRESTO_TOA_SNR_handle.close() #close the file, as it was open for writing and now we want to read it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the SNR and TOAs (with error) from the PRESTO output and save them into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESTO_TOA_SNR_handle = open('PRESTO_TOAs_SNRs.txt' , 'r')\n",
    "TOAs_handle = open('TOAs_TEMPO_format.txt', 'w') #the TOAs are saved as output by PRESTO, i.e. in a TEMPO format\n",
    "FFTFIT_results_handle = open('FFTFIT_results.txt', 'w')\n",
    "\n",
    "#to find floats, integers, and numbers with exponents in output files, we need to use regular expressions\n",
    "#this one was taken from https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string , second answer\n",
    "regexp_numeric_pattern = r'[-+]? (?: (?: \\d* \\. \\d+ ) | (?: \\d+ \\.? ) )(?: [Ee] [+-]? \\d+ ) ?'\n",
    "#this expression needs compiled by regexp\n",
    "any_number = re.compile(regexp_numeric_pattern, re.VERBOSE)\n",
    "\n",
    "\n",
    "for line in PRESTO_TOA_SNR_handle:\n",
    "    if 'SNR' in line:\n",
    "        b, b_error, SNR, SNR_error, _, _, _  = any_number.findall(line) \n",
    "        FFTFIT_results_handle.write(b+' '+b_error+' '+SNR+' '+SNR_error+'\\n')\n",
    "    if 'a                407.500' in line: #a is the TEMPO one-letter code from obsys.dat\n",
    "        TOAs_handle.write(line)\n",
    "        \n",
    "        \n",
    "TOA_list = np.genfromtxt('TOAs_TEMPO_format.txt')\n",
    "FFTFIT_results = np.genfromtxt('FFTFIT_results.txt')\n",
    "FFTFIT_results_with_MJDs = np.zeros((len(TOA_list[:,2]),np.size(FFTFIT_results,1)+1))\n",
    "FFTFIT_results_with_MJDs[0,:] = TOA_list\n",
    "FFTFIT_results_with_MJDs[:,1:np.size(FFTFIT_results,1)+2]= FFTFIT_results\n",
    "np.savetxt('FFTFIT_results.txt', FFTFIT_results_with_MJDs)\n",
    "TOAs_handle.close()\n",
    "PRESTO_TOA_SNR_handle.close()\n",
    "FFTFIT_results_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot SNR and TOA results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_list = FFTFIT_results[:,2]\n",
    "average_SNR = np.mean(SNR_list)\n",
    "log_handle.write('The average SNR is '+str(average_SNR)+'\\n')\n",
    "\n",
    "average_TOA_error = np.mean(TOA_list[:,3])\n",
    "log_handle.write('The average TOA error bar is '+str(average_TOA_error)+' microseconds.\\n')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = plt.gca()\n",
    "ax2.scatter(TOA_list[:,2], SNR_list, marker='o', markersize = 1, color='black')\n",
    "ax2.set_xlabel('TOA (MJD)')\n",
    "ax2.set_ylabel('SNR')\n",
    "plt.savefig('SNR_vs_TOA.pdf') \n",
    "plt.close()\n",
    "\n",
    "fig3 = plt.figure()\n",
    "ax3 = plt.gca()\n",
    "ax3.scatter(SNRs, TOA_list[:,3],  marker='o', color='black')\n",
    "ax3.set_xlabel('SNR')\n",
    "ax3.set_ylabel('TOA error bar ($\\mu$s)')\n",
    "plt.savefig('TOA_errors_vs_SNRs.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulsar timing fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
